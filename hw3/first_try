import torch
import torch.nn as nn
import torch.optim as optim
import gym
import numpy as np

class Memory:
   def __init__(self, buffer_size, state_dim):
      self.buffer_size = buffer_size
      self.circ_pointer = 0
      self.size = 0
      self.states = np.zeros((buffer_size, state_dim), dtype=np.float32)
      self.actions = np.zeros((buffer_size,), dtype=np.float32)
      self.rewards = np.zeros((buffer_size,), dtype=np.float32)
      self.next_states = np.zeros((buffer_size, state_dim), dtype=np.float32)
      self.dones = np.zeros((buffer_size,), dtype=np.float32)

   def add(self, state, action, reward, next_state, done):
      self.states[self.circ_pointer] = state
      self.actions[self.circ_pointer] = action
      self.rewards[self.circ_pointer] = reward
      self.next_states[self.circ_pointer] = next_state
      self.dones[self.circ_pointer] = done
      self.circ_pointer = (self.circ_pointer + 1) % self.buffer_size
      self.size = min(self.size + 1, self.buffer_size)

   def sample(self, batch_size):
      sampled = np.random.choice(self.size, batch_size, replace=False)
      return (
         self.states[sampled],
         self.actions[sampled],
         self.rewards[sampled],
         self.next_states[sampled],
         self.dones[sampled],
      )

class QNetwork(nn.Module):
   def __init__(self, state_dim, n_actions):
      super(QNetwork, self).__init__()

      self.fc1 = nn.Linear(state_dim, 128)
      self.fc2 = nn.Linear(128, 128)
      self.fc3 = nn.Linear(128, n_actions)  # Adjust the number of output units
      self.activation = nn.ReLU()

   def forward(self, state):
      q = self.fc1(state)
      q = self.activation(q)
      q = self.fc2(q)
      q = self.activation(q)
      q = self.fc3(q)

      return q


class DQN:
   def __init__(self, state_dim, n_actions, batch_size):
      self.state_dim = state_dim
      self.n_actions = n_actions
      self.batch_size = batch_size
      self.q_network = QNetwork(state_dim=state_dim, n_actions=n_actions)
      self.optimizer = optim.Adam(self.q_network.parameters(), lr=0.0005)
      self.memory = Memory(buffer_size=int(1e6), state_dim=state_dim)
      self.gamma = 0.99
      self.steps_done = 0
      self.epsilon_start = 1.0
      self.epsilon_end = 0.01
      self.epsilon_decay = 500
      self.slope = (self.epsilon_end - self.epsilon_start) / self.epsilon_decay

   def select_action(self, state):
      state = torch.from_numpy(state).float().unsqueeze(0)
      epsilon = max(self.steps_done * self.slope + self.epsilon_start, self.epsilon_end)
      if np.random.rand() > epsilon:
         with torch.no_grad():
            q_values = self.q_network.forward(state)
            action = torch.argmax(q_values, dim=1)
      else:
         action = np.random.randint(0, self.n_actions, (1,))

      return action.item()

   def update(self):
      if self.memory.size < self.batch_size:
         return  # Wait until enough samples are collected in the memory

      states, actions, rewards, next_states, dones = self.memory.sample(self.batch_size)

      states = torch.from_numpy(states).float()
      actions = torch.from_numpy(actions).long()
      rewards = torch.from_numpy(rewards).float()
      next_states = torch.from_numpy(next_states).float()
      dones = torch.from_numpy(dones).float()

      with torch.no_grad():
         next_actions = torch.argmax(self.q_network.forward(next_states), dim=1)
         target_q = rewards + (1 - dones) * self.gamma * self.q_network(next_states).gather(1, next_actions.unsqueeze(1))

      current_q = self.q_network(states).gather(1, actions.unsqueeze(1))

      loss = nn.functional.mse_loss(current_q, target_q.unsqueeze(1))

      self.optimizer.zero_grad()
      loss.backward()
      self.optimizer.step()

      self.steps_done += 1


         # Create LunarLander environment
env = gym.make("LunarLander-v2", render_mode="None")
state, _ = env.reset()
state_dim = len(state)
n_actions = env.action_space.n

# Create DQN agent
agent = DQN(state_dim=state_dim, n_actions=n_actions, batch_size=64)

# Training loop
for episode in range(1000):
   total_reward = 0
   state, _ = env.reset()

   while True:
      action = agent.select_action(state)
      next_state, reward, done, _, _ = env.step(action)
      agent.memory.add(state, action, reward, next_state, done)
      state = next_state
      total_reward += reward

      agent.update()

      if done:
         print(f"Episode: {episode + 1}, Total Reward: {total_reward}")
         break

env.close()


// call the trained ship
env = gym.make("LunarLander-v2", render_mode="human")
state, _ = env.reset()
state_dim = len(state)
n_actions = env.action_space.n


for episode in range(20):
   total_reward = 0
   state, _ = env.reset()

   while True:
    action = agent.select_action(state)
    next_state, reward, done, _, _ = env.step(action)
    agent.memory.add(state, action, reward, next_state, done)
    state = next_state
    total_reward += reward

    agent.update()

    if done:
        print(f"Episode: {episode + 1}, Total Reward: {total_reward}")
        break

env.close()
